{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e41d19-3a35-45cb-980c-cc0713b7dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries/packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889e70c-419c-4318-a361-ea3129743d16",
   "metadata": {},
   "source": [
    "User guide notes about data format\n",
    "(provided with dataset download):\n",
    "-----------------------------------\n",
    "\"Line 1…6 are useless in this dataset, and can be ignored. Points are described in following lines, one for each line.\n",
    "\n",
    "- Field 1: Latitude in decimal degrees.\n",
    "- Field 2: Longitude in decimal degrees.\n",
    "- Field 3: All set to 0 for this dataset.\n",
    "- Field 4: Altitude in feet (-777 if not valid).\n",
    "- Field 5: Date - number of days (with fractional part) that have passed since 12/30/1899.\n",
    "- Field 6: Date as a string.\n",
    "- Field 7: Time as a string.\n",
    "\n",
    "Note that field 5 and field 6&7 represent the same date/time in this dataset. You may use either of them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f618452-93e9-456a-a96b-5ac405a27162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for loading specific invidual PLT file given file_path\n",
    "def load_plt_file(file_path):\n",
    "\n",
    "    # load file as df while skipping first six rows (unneeded header info)\n",
    "    plt_df = pd.read_csv(file_path, skiprows = 6, header = None)\n",
    "\n",
    "    # assign columns\n",
    "    plt_df.columns = [\n",
    "        'latitude',\n",
    "        'longitude', \n",
    "        'zero_field',\n",
    "        'altitude_ft',\n",
    "        'date_days',\n",
    "        'date_string',\n",
    "        'time_string'\n",
    "    ]\n",
    "\n",
    "    return plt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c97d9c8-3239-403a-9245-666f50deec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for adding plt data to existing df\n",
    "def add_plt_data(df, file_path):\n",
    "\n",
    "    # use load_plt_file on given file and extract data\n",
    "    plt_df = load_plt_file(file_path)\n",
    "\n",
    "    # add plt data to existing dataframe\n",
    "    res_df = pd.concat([df, plt_df], axis = 0)\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913ae4a-f776-42a1-88e7-519de8519885",
   "metadata": {},
   "source": [
    "Filtering through all the folders and keeping track of the ones that have a 'labels.txt' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e7763ca-f709-428c-8ed3-d3748ca4e877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010',\n",
       " '020',\n",
       " '021',\n",
       " '052',\n",
       " '053',\n",
       " '056',\n",
       " '058',\n",
       " '059',\n",
       " '060',\n",
       " '062',\n",
       " '064',\n",
       " '065',\n",
       " '067',\n",
       " '068',\n",
       " '069',\n",
       " '073',\n",
       " '075',\n",
       " '076',\n",
       " '078',\n",
       " '080',\n",
       " '081',\n",
       " '082',\n",
       " '084',\n",
       " '085',\n",
       " '086',\n",
       " '087',\n",
       " '088',\n",
       " '089',\n",
       " '091',\n",
       " '092',\n",
       " '096',\n",
       " '097',\n",
       " '098',\n",
       " '100',\n",
       " '101',\n",
       " '102',\n",
       " '104',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '110',\n",
       " '111',\n",
       " '112',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '124',\n",
       " '125',\n",
       " '126',\n",
       " '128',\n",
       " '129',\n",
       " '136',\n",
       " '138',\n",
       " '139',\n",
       " '141',\n",
       " '144',\n",
       " '147',\n",
       " '153',\n",
       " '154',\n",
       " '161',\n",
       " '163',\n",
       " '167',\n",
       " '170',\n",
       " '174',\n",
       " '175',\n",
       " '179']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base directory holding the geolife data\n",
    "geolife_dir = \"../Data/GeoLife_Data\"\n",
    "\n",
    "# get all user folders within geolife directory\n",
    "user_folders = [f for f in os.listdir(geolife_dir)\n",
    "                if os.path.isdir(os.path.join(geolife_dir, f))\n",
    "                and f.isdigit()]\n",
    "\n",
    "# filter through the folders and check for labels.txt\n",
    "folders_with_labels = []\n",
    "for folder in user_folders:\n",
    "    labels_path = os.path.join(geolife_dir, folder, \"labels.txt\")\n",
    "    if os.path.exists(labels_path):\n",
    "        folders_with_labels.append(folder)\n",
    "\n",
    "folders_with_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76f602-4a1f-42cb-9b32-afe821e4ff7a",
   "metadata": {},
   "source": [
    "For reference, the below is the information included in the user guide that regards the mode transportation labels.\n",
    "--------------------------\n",
    "\"73 users have labeled their trajectories with transportation mode, such as driving, taking a bus, riding a bike and walking. There\n",
    "is a label file storing the transportation mode labels in each user’s folder. See section 4.2 for the format of labels.\n",
    "The total distance and duration of transportation modes are listed in Figure 6. Though this only covers a part of the dataset used\n",
    "in the following papers, the scale of this released dataset can still support transportation mode learning.\"\n",
    "\n",
    "Figure 6 Total distance and duration of transportation modes:\n",
    "| Transportation Mode | Distance (km) | Duration (hour) |\n",
    "| :------------------ | ------------: | --------------: |\n",
    "| Walk                |        10,123 |           5,460 |\n",
    "| Bike                |         6,495 |           2,410 |\n",
    "| Bus                 |        20,281 |           1,507 |\n",
    "| Car & taxi          |        32,866 |           2,384 |\n",
    "| Train               |        36,253 |             745 |\n",
    "| Airplane            |        24,789 |              40 |\n",
    "| Other               |         9,493 |             404 |\n",
    "| **Total**           |    **140,304**|       **12,953** |\n",
    "\n",
    "--------------------------\n",
    "\n",
    "\"Possible transportation modes are: walk, bike, bus, car, subway, train, airplane, boat, run and motorcycle. Again, we have\n",
    "converted the date/time of all labels to GMT, even though most of them were created in China.\n",
    "\n",
    "Example:\n",
    "| Start Time          | End Time            | Transportation Mode |\n",
    "| :------------------ | :------------------ | :------------------ |\n",
    "| 2008/04/02 11:24:21 | 2008/04/02 11:50:45 | bus                 |\n",
    "| 2008/04/03 01:07:03 | 2008/04/03 11:31:55 | train               |\n",
    "| 2008/04/03 11:32:24 | 2008/04/03 11:46:14 | walk                |\n",
    "| 2008/04/03 11:47:14 | 2008/04/03 11:55:07 | car                 |\n",
    "\n",
    "First, you can regard the label of both taxi and car as driving although we set them with different labels for future usage. Second, a\n",
    "user could label the transportation mode of a light rail as train while others may use subway as the label. Actually, no trajectory\n",
    "can be recorded in an underground subway system since a GPS logger cannot receive any signal there. In Beijing, the light rails\n",
    "and subway systems are seamlessly connected, e.g., line 13 (a light rail) is connected with line 10 and line 2, which are subway\n",
    "systems. Sometimes, a line (like line 5) is comprised of partial subways and partial light rails. So, users may have a variety of\n",
    "understanding in their transportation modes. You can differentiate the real train trajectories (connecting two cities) from the light\n",
    "rail trajectory (generating in a city) according to their distances. Or, just treat them the same.\"\n",
    "\n",
    "----------------------\n",
    "\n",
    "Now that we have a list of the folders in which the mode transportation labels are present and included, we can iterate through these folders and use the labels to filter for the relevant plt files (the ones labeled 'walk'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75639247-c650-4d0b-92df-dc1d6253c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 153/153 [26:44<00:00, 10.49s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 76/76 [05:57<00:00,  4.70s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  3.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 131/131 [10:07<00:00,  4.63s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:02<00:00,  5.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:06<00:00,  1.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 13.80it/s]\n",
      " 62%|█████████████████████████████████████████████████▉                              | 202/324 [41:59<25:21, 12.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     27\u001b[39m file_path = os.path.join(traj_path, file)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m df = \u001b[43mload_plt_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# make timestamp column\u001b[39;00m\n\u001b[32m     31\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mdate_string\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + df[\u001b[33m'\u001b[39m\u001b[33mtime_string\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mload_plt_file\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_plt_file\u001b[39m(file_path):\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# load file as df while skipping first six rows (unneeded header info)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     plt_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# assign columns\u001b[39;00m\n\u001b[32m      8\u001b[39m     plt_df.columns = [\n\u001b[32m      9\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtime_string\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     16\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\dementia_project\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\dementia_project\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\dementia_project\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\dementia_project\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\dementia_project\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:782\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# iterate through the folders with labels\n",
    "walk_plt_df = pd.DataFrame()\n",
    "\n",
    "for f_id in folders_with_labels:\n",
    "    f_path = os.path.join(geolife_dir, f_id)\n",
    "\n",
    "    # read labels.txt\n",
    "    labels_df = pd.read_csv(f\"{f_path}/labels.txt\", sep=\"\\t\")\n",
    "\n",
    "    # keep only \"walk\" labels\n",
    "    walk_df = labels_df[labels_df['Transportation Mode'] == 'walk']\n",
    "\n",
    "    for _, row in tqdm(walk_df.iterrows(), total=len(walk_df)):\n",
    "        # parse datetimes\n",
    "        start_dt = datetime.strptime(row['Start Time'], '%Y/%m/%d %H:%M:%S')\n",
    "        end_dt   = datetime.strptime(row['End Time'],   '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "        traj_path = os.path.join(f_path, \"Trajectory\")\n",
    "        if not os.path.exists(traj_path):\n",
    "            continue\n",
    "\n",
    "        # loop through all trajectory files\n",
    "        for file in os.listdir(traj_path):\n",
    "            if not file.endswith(\".plt\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(traj_path, file)\n",
    "            df = load_plt_file(file_path)\n",
    "\n",
    "            # make timestamp column\n",
    "            df['timestamp'] = pd.to_datetime(df['date_string'] + \" \" + df['time_string'])\n",
    "\n",
    "            # filter points inside the walk interval and make a copy\n",
    "            walk_points = df[(df['timestamp'] >= start_dt) & (df['timestamp'] <= end_dt)].copy()\n",
    "\n",
    "            if not walk_points.empty:\n",
    "                walk_points['user_id'] = f_id\n",
    "                walk_points['file_name'] = file\n",
    "                walk_plt_df = pd.concat([walk_plt_df, walk_points], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a1255-8180-4a89-9b47-e436c87b994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial basic data processing\n",
    "walk_plt_df['user_id'] = walk_plt_df['user_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b168f66-a9f2-4203-bcc7-6a222309e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an id column for differentiating sessions of traj data\n",
    "session_id_list = []\n",
    "user_session_id_list = []\n",
    "\n",
    "# initialize session counter + tracking variables\n",
    "session_id_count = 1\n",
    "user_session_id_count = 1\n",
    "current_user_id = 10\n",
    "current_file_name = \"20080331160008.plt\"\n",
    "\n",
    "# iterate through each row in plt data\n",
    "for idx, row in tqdm(walk_plt_df.iterrows(), total = len(walk_plt_df), desc = \"Assigning session IDs\"):\n",
    "\n",
    "    user_id = row['user_id']\n",
    "    file_name = row['file_name']\n",
    "    \n",
    "    # check if user_id changes\n",
    "    if user_id != current_user_id:\n",
    "        user_session_id_count = 1\n",
    "        session_id_count += 1\n",
    "        current_user_id = user_id\n",
    "        current_file_name = file_name\n",
    "    # check if file_name changes\n",
    "    elif file_name != current_file_name:\n",
    "        user_session_id_count += 1\n",
    "        session_id_count += 1\n",
    "        current_file_name = file_name\n",
    "    \n",
    "    user_session_id_list.append(user_session_id_count)\n",
    "    session_id_list.append(session_id_count)\n",
    "\n",
    "# add columns to dataframe\n",
    "walk_plt_df['user_session_id'] = user_session_id_list\n",
    "walk_plt_df['session_id'] = session_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275fc5d-4d49-414c-a0e6-484025fc20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to csv \n",
    "walk_plt_df.to_csv(r'../Data/plt_processed_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
